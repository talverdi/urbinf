{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Informatics\n",
    "# Module 10: Spatial weights and ESDA\n",
    "\n",
    "ESDA: Exploratory Spatial Data Analysis\n",
    "\n",
    "\"Everything is related to everything else, but near things are more related than distant things\" -Waldo Tobler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysal as ps\n",
    "import seaborn as sns\n",
    "from scipy.stats import stats\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tracts data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load geojson, display shape\n",
    "tracts_ca = gpd.read_file('census/census_tracts_data.geojson')\n",
    "tracts_ca = tracts_ca.set_index('index')\n",
    "tracts_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what variables are present?\n",
    "tracts_ca.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 5 rows\n",
    "tracts_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pop density in persons per sq km\n",
    "tracts_ca['pop_density'] = tracts_ca['total_pop'] / (tracts_ca['ALAND'] / 1e6)\n",
    "tracts_ca = tracts_ca.replace([np.inf, -np.inf], np.nan)\n",
    "tracts_ca = tracts_ca.dropna(subset=['pop_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project spatial geometries from lat-long to a meter-based projection for SoCal\n",
    "utm_ca = '+proj=utm +zone=11 +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "tracts_ca = tracts_ca.to_crs(utm_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA county only (and drop catalina island tracts)\n",
    "tracts = tracts_ca[tracts_ca['COUNTYFP']=='037'].drop(index=['06037599100', '06037599000'])\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory analysis\n",
    "\n",
    "Let's see what we've got with a couple key variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "tracts['pop_density'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "tracts['med_household_income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to choropleth map a df column\n",
    "def map_variable(df, col, scheme='quantiles', k=10, cmap='plasma', figsize=(10,10)):\n",
    "    values = df.dropna(subset=[col])\n",
    "    ax = values.plot(column=col, scheme=scheme, k=k, cmap=cmap, figsize=figsize)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map a variable\n",
    "map_variable(tracts, 'pop_density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map another variable\n",
    "map_variable(tracts, 'med_household_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these two variables might be negatively correlated? In general, where one is high, the other is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation coefficient and its p-value\n",
    "subset = tracts.dropna(subset=['pop_density', 'med_household_income'])\n",
    "x = subset['pop_density']\n",
    "y = subset['med_household_income']\n",
    "r, p = stats.pearsonr(x=x, y=y)\n",
    "print('r={:.4f}, p={:.4f}'.format(r, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot them with matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=x, y=y, s=1)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model with scipy\n",
    "m, b, r, p, se = stats.linregress(x=x, y=y)\n",
    "print('m={:.4f}, b={:.4f}, r^2={:.4f}, p={:.4f}'.format(m, b, r ** 2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a log-log simple linear regression model with scipy\n",
    "m, b, r, p, se = stats.linregress(x=np.log(x), y=np.log(y))\n",
    "print('m={:.4f}, b={:.4f}, r^2={:.4f}, p={:.4f}'.format(m, b, r ** 2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-log regression line with 95% CI using seaborn\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax = sns.regplot(np.log(x), np.log(y), marker='.', scatter_kws={'s':2}, ax=ax)\n",
    "ax.set_xlim((3, 11))\n",
    "ax.set_ylim((9.5, 12.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# look through the list of columns, pick two new variables, inspect their descriptive stats\n",
    "# then calculate their correlation coefficient and estimate a simple linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial weights matrix\n",
    "\n",
    "Spatial weights define the spatial connections among our units of analysis (tracts in this case).\n",
    "\n",
    "### 2.1. Contiguity-based weights: rook contiguity\n",
    "\n",
    "Using rook contiguity, two spatial units must share an edge of their boundaries to be considered neighbors. This isn't terribly common in practice (since queen is usually more useful, but it's worth understanding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tract labels (GEOIDs) and pick one (arbitrarily) to work with later\n",
    "labels = tracts.index.tolist()\n",
    "label = labels[603]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate rook spatial weights\n",
    "w_rook = ps.lib.weights.Rook.from_dataframe(tracts, ids=labels, id_order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neighbors of some tract\n",
    "w_rook.neighbors[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Contiguity-based weights: queen contiguity\n",
    "\n",
    "Using queen contiguity, two spatial units need only share a vertex (a single point) of their boundaries to be considered neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate queen spatial weights\n",
    "w_queen = ps.lib.weights.Queen.from_dataframe(tracts, ids=labels, id_order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neighbors of some tract\n",
    "w_queen.neighbors[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a raw contiguity matrix, so weights are binary 1s and 0s meaning neighbor/not\n",
    "w_queen.weights[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many neighbors does this tract have?\n",
    "w_queen.cardinalities[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cardinalities to series and describe data\n",
    "cardinalites_queen = pd.Series(w_queen.cardinalities)\n",
    "cardinalites_queen.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of number of neighbors\n",
    "ax = cardinalites_queen.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "w_queen.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of neighbors\n",
    "w_queen.mean_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of neighbors\n",
    "w_queen.min_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of neighbors\n",
    "w_queen.max_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# islands are observations with no neighbors, disconnected in space (can cause modeling problems)\n",
    "w_queen.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a census tract of interest, along with its neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tracts.plot(ax=ax, facecolor='#999999', edgecolor='w', linewidth=0.5)\n",
    "\n",
    "# plot some tract of interest in red\n",
    "tract = tracts.loc[[label]]\n",
    "tract.plot(ax=ax, facecolor='r', edgecolor='w', linewidth=1)\n",
    "\n",
    "# plot the neighbors in blue\n",
    "neighbors = tracts.loc[w_queen[label]]\n",
    "neighbors.plot(ax=ax, facecolor='b', edgecolor='w', linewidth=1)\n",
    "\n",
    "# zoom to area of interest\n",
    "xmin, ymin, xmax, ymax = neighbors.unary_union.bounds\n",
    "ax.axis('equal')\n",
    "ax.set_xlim(xmin*0.999, xmax*1.001)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.set_title('Neighbors of tract {}'.format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# draw a contiguity graph of the tracts\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "tracts.plot(ax=ax, facecolor='#333333', edgecolor='w', linewidth=0.2)\n",
    "\n",
    "for lbl, neighbors in w_queen:\n",
    "    tract_point = np.hstack(tracts.loc[lbl, 'geometry'].centroid.xy)\n",
    "    neighbor_points = np.vstack(tracts.loc[neighbors, 'geometry'].apply(lambda g: (g.centroid.x, g.centroid.y)).values)\n",
    "    for neighbor_point in neighbor_points:\n",
    "        ax.plot(*zip(tract_point, neighbor_point), color='r', linewidth=0.3)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Distance-based weights: *k*-nn\n",
    "\n",
    "Find the *k*-nearest neighbors of each tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# k-nearest neighbors finds the closest k tract centroids to each tract centroid\n",
    "w_knn = ps.lib.weights.KNN.from_dataframe(tracts, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they all have exactly k neighbors\n",
    "w_knn.neighbors[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Distance-based weights: distance band\n",
    "\n",
    "Here tracts are considered neighbors of some tract if they are within a given threshold distance of it. Distance band weights can be specified to take on continuous values rather than binary (1s and 0s), with these values being the inverse distance between each pair of \"neighboring\" units. The default (linear) distance-decay exponent is thus -1. If we set it to -2, we get gravity model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the minimum nearest neighbor distance so each unit is assured of at least one neighbor\n",
    "x = tracts.centroid.x\n",
    "y = tracts.centroid.y\n",
    "coords = np.array([x, y]).T\n",
    "threshold = ps.lib.weights.min_threshold_distance(coords)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# default alpha=-1 (linear decay) and gravity weights alpha=-2\n",
    "w_dist = ps.lib.weights.distance.DistanceBand.from_dataframe(tracts, threshold=threshold, binary=False, alpha=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many distance-band neighbors does our tract have?\n",
    "len(w_dist.neighbors[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map the neighbors, colored by weight from nearest to furthest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tracts.plot(ax=ax, facecolor='k', edgecolor='gray', linewidth=1)\n",
    "\n",
    "# get the tract of interest and its neighbors/weights\n",
    "tract = tracts.loc[[label]]\n",
    "weights = pd.Series(w_dist[label])\n",
    "neighbors = tracts.loc[weights.index, ['geometry']]\n",
    "\n",
    "# plot the tract's neighbors in blues by weight\n",
    "neighbors['weights_scaled'] = weights\n",
    "neighbors.plot(ax=ax, column='weights_scaled', cmap='Blues_r', edgecolor='gray', linewidth=1, scheme='Fisher_Jenks')\n",
    "\n",
    "# plot the tract of interest in white\n",
    "tract.plot(ax=ax, facecolor='w', edgecolor='r', linewidth=2)\n",
    "\n",
    "# zoom to area of interest\n",
    "xmin, ymin, xmax, ymax = neighbors.unary_union.bounds\n",
    "ax.axis('equal')\n",
    "ax.set_xlim(xmin*0.99, xmax*1.01)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_title('Neighbors of tract {}'.format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# recompute the distance-based spatial weights matrix using different threshold and alpha values\n",
    "# how does this impact the number of neighbors and the map above?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Standardizing weights\n",
    "\n",
    "A spatial weights matrix with raw values (e.g., binary 1s and 0s for neighbor/not) is not always the best for analysis. Some sort of standardization is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the neighbors and weights of our tract\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current transformation of the weights matrix (O = original)\n",
    "w_queen.get_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we want to apply a row-based transformation, so every row of the matrix sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the queen weights\n",
    "w_queen.set_transform('R')\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the linear-decay distance-based weights\n",
    "w_dist.set_transform('R')\n",
    "#w_dist[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySAL supports the following transformations:\n",
    "\n",
    "  - O: original, returning the object to the initial state\n",
    "  - B: binary, with every neighbor having assigned a weight of 1\n",
    "  - R: row-based, with all the neighbors of a given observation adding up to 1\n",
    "  - V: variance stabilizing, with the sum of all the weights being constrained to the number of observations\n",
    "\n",
    "**It can take a long time to calculate a weights matrix for a large data set.**\n",
    "\n",
    "Once you've created yours, you might want to save it to disk to re-use in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your matrix to disk\n",
    "f = ps.lib.io.open('tracts_queen.gal', 'w')\n",
    "f.write(w_queen)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a matrix from disk (notice its transformation)\n",
    "w_queen = ps.lib.io.open('tracts_queen.gal', 'r').read()\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial lag\n",
    "\n",
    "Spatial lag tells us about values near some spatial unit. Calculate spatial lag of the `med_household_income` variable. If the spatial weights matrix is row-standardized, then the spatial lag is the average value of an observation's neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a variable to investigate\n",
    "col = 'med_household_income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first drop null rows\n",
    "tracts_not_null = tracts[[col, 'geometry']].dropna()\n",
    "y = tracts_not_null[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute spatial weights for these observations then row-standardize\n",
    "w_queen = ps.lib.weights.Queen.from_dataframe(tracts_not_null)\n",
    "w_queen.set_transform('R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial lag\n",
    "y_lag = ps.lib.weights.lag_spatial(w_queen, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is a tract's med income similar to those of its neighbors?\n",
    "col_lag = '{}_lag'.format(col)\n",
    "data_lag = pd.DataFrame(data={col:y, col_lag:y_lag}).astype(int)\n",
    "data_lag.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# how do the lagged values change if we use a distance-based spatial weights matrix instead?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial autocorrelation\n",
    "\n",
    "Statistical models typically assume that the observations are independent of each other. This assumption is violated when a variable's value at one location is correlated with its value at nearby locations. This is called spatial autocorrelation, and is common in the real world due to proximity-based spillover effects. Substantive spatial autocorrelation can be explained by social or economic theory that describes a spatial relationship. Nuisance spatial autocorrelation stems from data problems.\n",
    "\n",
    "*Positive spatial autocorrelation*: nearby values tend to be more similar (e.g. income, home values, temperature, rainfall)\n",
    "\n",
    "*Negative spatial autocorrelation*: nearby values tend to be more dissimilar (e.g. fire stations, grocery stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Moran's I\n",
    "\n",
    "Moran's I measures *global* spatial autocorrelation: do values tend to be near other (dis)similar values. Values > 0 indicate positive spatial autocorrelation, and values < 0 indicate negative spatial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistic\n",
    "mi = ps.explore.esda.Moran(data_lag[col], w_queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the I value\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical inference: show the p value\n",
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we generated a large number of maps with the same values but randomly allocated over space, and calculated Moran's I for each of these maps, only 1/1000 of them would display a larger absolute value than the one we computed from the real-world data set. Thus there is a 1/1000 chance of getting the observed value of Moran's I if the spatial distribution of our variable is random. We can conclude that the variable's distribution is statistically-significantly postively spatially autocorrelated. We'll talk more about inference in the next module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# calculate the moran's I of median home values\n",
    "# is it statistically significant?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Moran plots\n",
    "\n",
    "A Moran plot scatter plots the spatially-lagged values (y-axis) vs the original variable's values (x-axis). Moran's I equals the slope of the line in a Moran plot, which makes this all a bit easier to conceptualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.regplot(x=col, y=col_lag, data=data_lag, scatter_kws={'s':1})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the vector (i.e., calculate z-scores)\n",
    "y_std = (y - y.mean()) / y.std()\n",
    "y_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the spatial lag of the standardized vector and save it as a series with same index\n",
    "y_std_lag = pd.Series(ps.lib.weights.lag_spatial(w_queen, y_std), index=y_std.index, name=col_lag)\n",
    "y_std_lag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized moran's plot, ignoring outliers beyond 3 std devs\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "sns.regplot(ax=ax, x=y_std, y=y_std_lag, scatter_kws={'s':1})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the 95% confidence interval shading and the positive slope. Given the value of Moran's I that we calculated earlier (and its p-value), we can conclude that the slope of the line is statistically-significantly different from zero. This plot makes it visually apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model (of the line above)\n",
    "m, b, r, p, se = stats.linregress(x=y_std, y=y_std_lag)\n",
    "print('m={:.4f}, b={:.4f}, r^2={:.4f}, p={:.4f}'.format(m, b, r ** 2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the slope is the same as moran's I, calculated earlier\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# visualize a standardized moran's plot of median home values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. LISAs\n",
    "\n",
    "Local Indicators of Spatial Autocorrelation: are there specific areas with high concentrations of (dis)similar values?\n",
    "\n",
    "Moran's I tells us about spatial clustering across the data set as a whole. However, it does not tell us where these clusters occur. For that, we need a local measure. Essentially, we will classify the data set's observations into four groups based on the four quadrants of the Moran plot:\n",
    "\n",
    "  1. **HH**: high value near other high values (*hot spots*)\n",
    "  1. **LL**: low value near other low values (*cold spots*)\n",
    "  1. **HL**: high value near low values (*spatial outliers*)\n",
    "  1. **LH**: low value near high values (*spatial outliers*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized moran's plot again, from subsection above only labeled this time\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.axvline(0, c='k', ls=':', alpha=0.5)\n",
    "ax.axhline(0, c='k', ls=':', alpha=0.5)\n",
    "ax.text(1.25, 1.25, 'HH', fontsize=20)\n",
    "ax.text(1.25, -1.75, 'HL', fontsize=20)\n",
    "ax.text(-1.75, 1.25, 'LH', fontsize=20)\n",
    "ax.text(-1.75, -1.75, 'LL', fontsize=20)\n",
    "ax.set_title('Moran Plot')\n",
    "sns.regplot(ax=ax, x=y_std, y=y_std_lag, scatter_kws={'s':1})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate LISA values using the queen spatial weights\n",
    "lisa = ps.explore.esda.Moran_Local(data_lag[col], w_queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the statistical-significance threshold (alpha)\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify whether each observation is significant or not\n",
    "# p-value interpretation same as earlier with moran's I\n",
    "data_lag['significant'] = lisa.p_sim < alpha\n",
    "data_lag['significant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the quadrant each observation belongs to\n",
    "data_lag['quadrant'] = lisa.q\n",
    "data_lag['quadrant'] = data_lag['quadrant'].replace({1:'HH', 2:'LH', 3:'LL', 4:'HL'})\n",
    "data_lag['quadrant'].sort_values().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now map the tracts, colored according to their LISA quadrants, to identify clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the original tracts and LISA quadrants data together\n",
    "tracts_lisa = gpd.GeoDataFrame(pd.merge(tracts, data_lag, how='left', left_index=True, right_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axis then draw the basemap of tracts\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "tracts_lisa.plot(ax=ax, facecolor='#999999', edgecolor='k', linewidth=0.1)\n",
    "\n",
    "# plot each quandrant's tracts (if significant LISA statistic) in a different color\n",
    "quadrant_colors = {'HH':'r', 'LL':'b', 'LH':'skyblue', 'HL':'pink'}\n",
    "for q, c in quadrant_colors.items():\n",
    "    mask = tracts_lisa['significant'] & (tracts_lisa['quadrant']==q)\n",
    "    rows = tracts_lisa.loc[mask]\n",
    "    rows.plot(ax=ax, color=c, edgecolor='k', linewidth=0.1)\n",
    "\n",
    "ax.axis('off')\n",
    "fig.savefig('clusters.png', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gray tracts have statistically-insignificant LISA values. In red we see clusters of tracts with high values surrounded by other high values. In blue we see clusters of tracts with low values surrounded by other low values. In pink, we see the first type of spatial outliers: tracts with high values but surrounded by low values. Finally, in light blue we see the other type of spatial outlier: tracts with low values surrounded by other tracts with high values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice exploratory spatial analysis, do the following:\n",
    "\n",
    "  1. Select the tracts in a different CA county\n",
    "  1. Calculate a new spatial weights matrix for this subset\n",
    "  1. Choose a new variable from the data set\n",
    "  1. Calculate its Moran's I\n",
    "  1. Visualize its Moran's plot\n",
    "  1. Calculate and map its LISA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (urbinf)",
   "language": "python",
   "name": "urbinf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
