{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Informatics\n",
    "# Module 11: Inferential Spatial Modeling\n",
    "\n",
    "Statistical inference is the process of using a sample to *infer* the characteristics of an underlying population (from which this sample was drawn) through estimation and hypothesis testing. Contrast this with descriptive statistics, which focus simply on describing the characteristics of the sample itself.\n",
    "\n",
    "Common goals of inferential statistics include:\n",
    "\n",
    "  - parameter estimation and confidence intervals\n",
    "  - hypothesis rejection\n",
    "  - prediction\n",
    "  - model selection\n",
    "\n",
    "**Theory and Models**\n",
    "\n",
    "\"Theories are structures of ideas that explain and interpret facts.\" -Stephen Jay Gould\n",
    "\n",
    "To conduct statistical inference, we rely on *statistical models*: sets of assumptions plus mathematical relationships between variables, producing a formal representation of some theory. We are essentially trying to explain the process underlying the generation of our data. What is the probability distribution (the probabilities of occurrence of different possible outcome values of our response variable)?\n",
    "\n",
    "***Spatial* inference** introduces explicit spatial relationships into the statistical modeling framework, as both theory-driven (e.g., spatial spillovers) and data-driven (e.g., MAUP) issues could otherwise violate modeling assumptions.\n",
    "\n",
    "Schools of statistical inference:\n",
    "\n",
    "  - frequentist\n",
    "    - frequentists think of probability as proportion of time some outcome occurs (relative frequency)\n",
    "    - given lots of repeated trials, how likely is the observed outcome?\n",
    "    - concepts: statistical hypothesis testing, *p*-values, confidence intervals\n",
    "  - bayesian\n",
    "    - bayesians think of probability as amount of certainty observer has about an outcome occurring (subjective probability)\n",
    "    - probability as a measure of how much info the observer has about the real world, updated as info changes\n",
    "    - concepts: prior probability, likelihood, bayes' rule, posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysal as ps\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "tracts_ca = gpd.read_file('data/census_tracts_data.geojson').set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA, ventura, orange counties only (and drop offshore island tracts)\n",
    "islands = ['06037599100', '06037599000', '06111980000', '06111990100', '06111003612']\n",
    "tracts = tracts_ca[tracts_ca['COUNTYFP'].isin(['037', '059', '111'])].drop(index=islands)\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project spatial geometries from lat-long to a meter-based projection for SoCal\n",
    "utm_ca = '+proj=utm +zone=11 +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "tracts = tracts.to_crs(utm_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the data\n",
    "ax = tracts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical inference: introduction\n",
    "\n",
    "### 1a. Estimating population parameters\n",
    "\n",
    "Here, our population = all tracts in Orange, Ventura, and LA counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stat of the population: average tract-level median income\n",
    "tracts['med_household_income'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stat of a simple random sample\n",
    "n = 500\n",
    "sample = tracts['med_household_income'].sample(n)\n",
    "sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar is our sample mean to our population mean? Is it a good estimate? We have calculated a \"point estimate\" of the population mean. Let's calculate an \"interval estimate\" of it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confidence interval using t-distribution (bc population std dev is unknown)\n",
    "sample = sample.dropna() #drop nulls\n",
    "conf = 0.95 #confidence level\n",
    "df = len(sample) - 1 #degrees of freedom\n",
    "mean = sample.mean() #the sample's mean\n",
    "sem = stats.sem(sample) #the standard error of the mean\n",
    "lower, upper = stats.t.interval(conf, df, loc=mean, scale=sem)\n",
    "\n",
    "# calculate the margin of error\n",
    "moe = upper - sample.mean()\n",
    "\n",
    "# display confidence interval\n",
    "print(f'{lower:0.0f} – {upper:0.0f} ({conf*100:0.0f}% confidence interval)')\n",
    "print(f'{mean:0.0f} ± {moe:0.0f} (at {conf*100:0.0f}% confidence level)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are 95% confident that this interval contains the true population parameter value. That is, if we were to repeat this process many times (sampling then computing CI), on average 95% of the CIs would contain the true population parameter value (and 5% wouldn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try different sample sizes and alpha levels: how do these change the confidence interval's size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# randomly sample 100 tract-level median home values then calculate the mean and 99% confidence interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. *t*-tests: difference in means\n",
    "\n",
    "Is the difference between two groups statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a variable\n",
    "var = 'med_home_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two data subsets\n",
    "black_tracts = tracts[tracts['pct_black'] > 50]\n",
    "group1 = black_tracts[var]\n",
    "hispanic_tracts = tracts[tracts['pct_hispanic'] > 50]\n",
    "group2 = hispanic_tracts[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the probability distributions of these two data sets?\n",
    "fig, ax = plt.subplots()\n",
    "ax = group1.plot.kde(ls='--', c='k', alpha=0.5, lw=2, bw_method=0.7)\n",
    "ax = group2.plot.kde(ls='-', c='k', alpha=0.5, lw=2, bw_method=0.7, ax=ax)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(group1.mean()))\n",
    "print(int(group2.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in means\n",
    "diff = group1.mean() - group2.mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the t-stat and its p-value\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2, equal_var=False, nan_policy='omit')\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the difference in means statistically significant?\n",
    "alpha = 0.05 #significance level\n",
    "p_value < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what is the difference in mean tract-level median home values in majority white vs majority black tracts?\n",
    "# is it statistically significant?\n",
    "# what if you randomly sample just 25 tracts from each group: is their difference significant?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical models\n",
    "\n",
    "Introduction to OLS linear regression.\n",
    "\n",
    "Lots to cover in a course on regression that we must skip for today's quick overview. But in general you'd want to:\n",
    "\n",
    "  - specify a model (or alternative models) based on theory\n",
    "  - inspect candidate predictors' relationships with the response\n",
    "  - inspect the predictors' relationships with each other (and reduce multicollinearity)\n",
    "  - transform predictors for better linearity\n",
    "  - identify and handle outlier observations\n",
    "  - regression diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Simple (bivariate) linear regression\n",
    "\n",
    "OLS regression with a single predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response variable and drop any rows in which it is null\n",
    "response = 'med_home_value'\n",
    "tracts = tracts.dropna(subset=[response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable vector\n",
    "predictors = 'med_household_income'\n",
    "X = tracts[predictors].dropna()\n",
    "y = tracts.loc[X.index][response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model with scipy\n",
    "m, b, r, p, se = stats.linregress(x=X, y=y)\n",
    "print('m={:.4f}, b={:.4f}, r^2={:.4f}, p={:.4f}'.format(m, b, r ** 2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model with statsmodels\n",
    "Xc = add_constant(X)\n",
    "model = sm.OLS(y, Xc)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single predictor explains about half the variation of the response. To explain more, we need more predictors.\n",
    "\n",
    "### 2b. Multiple regression\n",
    "\n",
    "OLS regression with multiple predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable vector\n",
    "predictors = ['med_household_income', 'pct_white']\n",
    "X = tracts[predictors].dropna()\n",
    "y = tracts.loc[X.index][response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model\n",
    "Xc = add_constant(X)\n",
    "model = sm.OLS(y, Xc)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add in more variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable vector\n",
    "predictors = ['pct_white', 'pct_built_before_1940', 'med_rooms_per_home', 'pct_bachelors_degree']\n",
    "X = tracts[predictors].dropna()\n",
    "y = tracts.loc[X.index][response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model\n",
    "Xc = add_constant(X)\n",
    "model = sm.OLS(y, Xc)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try different sets of predictors to increase R-squared while keeping the total number of predictors relatively low and theoretically sound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Standardized regression\n",
    "\n",
    "*Beta coefficients* are the estimated regression coefficients when the response and predictors are standardized so that their variances equal 1. Thus, we can interpret these coefficients as how many standard deviations the response changes for each standard deviation increase in the predictor. This tells us about \"effect size\": which predictors have greater effects on the response by ignoring the variables' different units/scales of measurement. However, it relies on the variables' distributions having similar shapes (otherwise the meaning of a std dev in one will differ from a std dev in another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a standardized regression model\n",
    "y_stdrd = pd.Series(stats.mstats.zscore(y), index=y.index, name=y.name)\n",
    "X_stdrd = pd.DataFrame(stats.mstats.zscore(X), index=X.index, columns=X.columns)\n",
    "Xc_stdrd = add_constant(X_stdrd)\n",
    "model_stdrd = sm.OLS(y_stdrd, Xc_stdrd)\n",
    "result_stdrd = model_stdrd.fit()\n",
    "print(result_stdrd.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Diagnostics\n",
    "\n",
    "Let's take a step back and think about some of the steps we might take prior to specifying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "# how well are predictors correlated with response... and with each other?\n",
    "correlations = tracts[[response] + sorted(predictors)].corr()\n",
    "correlations.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual correlation matrix via seaborn heatmap\n",
    "# use vmin, vmax, center to set colorbar scale properly\n",
    "sns.set(style='white')\n",
    "ax = sns.heatmap(correlations, vmin=-1, vmax=1, center=0,\n",
    "                 cmap=plt.cm.coolwarm, square=True, linewidths=1)\n",
    "ax.set_ylim(plt.ylim()[0] + 0.5, plt.ylim()[1] - 0.5) #fix temporary mpl bug\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairwise relationships with seaborn\n",
    "grid = sns.pairplot(tracts[[response] + sorted(predictors)], markers='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spatial models\n",
    "\n",
    "Basic types:\n",
    "\n",
    "  - **Spatial heterogeneity**: account for systematic differences across space without explicitly modeling interdependency (non-spatial estimation)\n",
    "    - spatial fixed effects (intercept varies for each spatial group)\n",
    "    - spatial regimes (intercept and coefficients vary for each spatial group)\n",
    "  - **Spatial dependence**: model interdependencies between observations through space\n",
    "    - spatial lag model (spatially-lagged endogenous variable added as predictor;  because of endogeneity, cannot use OLS to estimate)\n",
    "    - spatial error model (spatial effects in error term)\n",
    "    - spatial lag+error combo model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3, we'll look at models that account for spatial heterogeneity. In section 4, we'll look at models that account for spatial dependence.\n",
    "\n",
    "### 3a. Spatial fixed effects\n",
    "\n",
    "Using dummy variables representing the counties into which our observations (tracts) are nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dummy variable for each county, with 1 if tract is in this county and 0 if not\n",
    "for county in tracts['COUNTYFP'].unique():\n",
    "    new_col = f'dummy_county_{county}'\n",
    "    tracts[new_col] = (tracts['COUNTYFP'] == county).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one dummy from dummies to prevent perfect collinearity\n",
    "# ie, a subset of predictors sums to 1 (which full set of dummies will do)\n",
    "county_dummies = [f'dummy_county_{county}' for county in tracts['COUNTYFP'].unique()]\n",
    "county_dummies = county_dummies[:-1]\n",
    "county_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable vector\n",
    "X = tracts[predictors + county_dummies].dropna()\n",
    "y = tracts.loc[X.index][response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model\n",
    "Xc = add_constant(X)\n",
    "model = sm.OLS(y, Xc)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Spatial regimes\n",
    "\n",
    "Each spatial regime can have different model coefficients. Here, the regimes are our 3 counties. This subsection just uses OLS for estimation, but you can also combine spatial regimes with spatial autogression models (the latter is introduced later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), a response variable matrix, and a regimes vector\n",
    "X = tracts[predictors].dropna() #only take rows with non-null observations\n",
    "Y = tracts.loc[X.index][[response]] #notice this is a matrix (not a vector) this time for pysal\n",
    "regimes = tracts.loc[X.index]['COUNTYFP'] #define the regimes\n",
    "regimes.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate spatial regimes model with OLS\n",
    "olsr = ps.model.spreg.OLS_Regimes(y=Y.values, x=X.values, regimes=regimes.values, name_regimes='county',\n",
    "                                  name_x=X.columns.tolist(), name_y=response, name_ds='tracts')\n",
    "print(olsr.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Dependence\n",
    "\n",
    "### 4a. Spatial diagnostics\n",
    "\n",
    "So far we've seen two spatial heterogeneity models. Now we'll explore spatial dependence, starting by using queen-contiguity spatial weights to model spatial relationships between observations and OLS to check diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable matrix\n",
    "X = tracts[predictors].dropna()\n",
    "Y = tracts.loc[X.index][[response]] #notice this is a matrix this time for pysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial weights from tract geometries (but only those tracts that appear in design matrix!)\n",
    "W = ps.lib.weights.Queen.from_dataframe(tracts.loc[X.index])\n",
    "W.transform = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OLS spatial diagnostics to check the nature of spatial dependence\n",
    "ols = ps.model.spreg.OLS(y=Y.values, x=X.values, w=W, spat_diag=True, moran=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate moran's I (for the response) and its significance\n",
    "mi = ps.explore.esda.Moran(y=Y, w=W, two_tailed=True)\n",
    "print(mi.I)\n",
    "print(mi.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran's I (for the residuals): moran's i, standardized i, p-value\n",
    "ols.moran_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "A significant Moran's *I* suggests spatial autocorrelation, but doesn't tell us which alternative specification should be used. Lagrange Multiplier (LM) diagnostics can help with that. If one LM test is significant and the other isn't, then that tells us which model specification (spatial lag vs spatial error) to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.lm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial error model: stat, p\n",
    "ols.lm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "If (and only if) both the LM tests produce significant statistics, try the robust versions (the nonrobust LM tests are sensitive to each other):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.rlm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial error model: stat, p\n",
    "ols.rlm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So... which model specification to choose?\n",
    "\n",
    "If neither LM test is significant: use regular OLS.\n",
    "\n",
    "If only one LM test is significant: use that model spec.\n",
    "\n",
    "If both LM tests are significant: run robust versions.\n",
    "\n",
    "If only one robust LM test is significant: use that model spec.\n",
    "\n",
    "If both robust LM tests are significant (this can often happen with large sample sizes):\n",
    "\n",
    "  - first consider if the initial model specification is actually a good fit\n",
    "  - if so, use the spatial specification corresponding to the larger robust-LM statistic\n",
    "  - or consider a combo model\n",
    "\n",
    "### 4b. Spatial lag model\n",
    "\n",
    "When the diagnostics indicate the presence of a spatial diffusion process.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + X \\beta + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ vector of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mll = ps.model.spreg.ML_Lag(y=Y.values, x=X.values, w=W, method='full', name_w='queen',\n",
    "                            name_x=X.columns.tolist(), name_y=response, name_ds='tracts')\n",
    "print(mll.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, rho\n",
    "mll.rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Spatial error model\n",
    "\n",
    "When the diagnostics indicate the presence of spatial error dependence.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = X \\beta + u$\n",
    "\n",
    "where $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors. The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mle = ps.model.spreg.ML_Error(y=Y.values, x=X.values, w=W, method='full', name_w='queen',\n",
    "                            name_x=X.columns.tolist(), name_y=response, name_ds='tracts')\n",
    "print(mle.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, lambda\n",
    "mle.lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Spatial lag+error combo model\n",
    "\n",
    "Estimated with GMM (generalized method of moments). Essentially a spatial error model with endogenous explanatory variables.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + X \\beta + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ vector of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors.\n",
    "\n",
    "The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmc = ps.model.spreg.GM_Combo_Het(y=Y.values, x=X.values, w=W, name_w='queen', name_ds='tracts',\n",
    "                                  name_x=X.columns.tolist(), name_y=response)\n",
    "print(gmc.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# with a new set of predictors, compute spatial diagnostics and estimate a new spatial model accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (urbinf)",
   "language": "python",
   "name": "urbinf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
