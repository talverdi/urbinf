{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Informatics\n",
    "# Module 04: Intro to pandas, Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# consistent randomization\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selecting and slicing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation                       Syntax           Result\n",
    "#------------------------------------------------------------\n",
    "# Select column                   df[col]          Series\n",
    "# Select row by label             df.loc[label]    Series\n",
    "# Select row by integer location  df.iloc[loc]     Series\n",
    "# Slice rows                      df[5:10]         DataFrame\n",
    "# Select rows by boolean vector   df[mask]         DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe from the location data set\n",
    "df = pd.read_csv('data/gps-coords.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing by row/column label(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a column from the dataframe by its label with [] indexing - returns column as a pandas series with the dataframe's index\n",
    "df['city'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get multiple columns by their labels by passing a list of column names within the [] operator - returns a dataframe (subset)\n",
    "li = ['city', 'country']\n",
    "df[li].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single value using the [] operator for a column label then a row label\n",
    "df['city'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing using .loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to select by row label - returns the row as a series whose index is the dataframe column labels\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to select by row label and column label\n",
    "df.loc[0, 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:3 is a slice of rows with label 1 to label 3\n",
    "# ['city', 'date'] is a list of column labels\n",
    "df.loc[1:3, ['city', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice by rows and columns\n",
    "df.loc[1:3, 'date':'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set values with .loc as well\n",
    "print(df.loc[0, 'city'])\n",
    "df.loc[0, 'city'] = 'London'\n",
    "print(df.loc[0, 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing using .iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .iloc for integer-position based indexing as [row, column]\n",
    "# get the value from the row in position 3 and the column in position 2 (zero-indexed)\n",
    "df.iloc[3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use .iloc with slices too\n",
    "# slice rows from position 112 to 115 and columns from position 2 to 4\n",
    "# iloc is not inclusive, so for example \"from 2 to 4\" will return positions 2 and 3 (but not 4)\n",
    "df.iloc[112:115, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use iloc to sample every nth row from a data set\n",
    "n = 300\n",
    "df.iloc[range(0, len(df), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's easier to tell the difference between .loc and .iloc if the index labels aren't the same as their positions\n",
    "df.index = [label**2 for label in df.index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns the rows with labels between 4 and 9 (.loc is inclusive)\n",
    "df.loc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns rows with labels in positions 4 through 8 (not through 9, because .iloc is not inclusive)\n",
    "df.iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# select the even-numbered rows (by integer position) and all columns that begin with \"c\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping and summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average latitude observed per city\n",
    "groups['lat'].mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average latitude observed in the 10 cities with the most observations\n",
    "most_observed = groups.size().sort_values(ascending=False).index\n",
    "groups['lat'].mean().reindex(most_observed).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# calculate the single westernmost coordinate in each country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select rows by value\n",
    "\n",
    "We introduced this briefly in the previous module, but now we'll explore this topic in more depth, including using logical operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a reduced set of gps data\n",
    "df = pd.read_csv('data/gps-coords-reduced.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Series of true/false, indicating if each row in the column is equal to some value\n",
    "(df['city']=='Munich').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, select only those rows in the df that match that condition\n",
    "df[df['city']=='Munich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas logical operators are: | for or, & for and, ~ for not\n",
    "# these must be grouped by using parentheses\n",
    "# which observations are in spain but not in barcelona?\n",
    "not_bcn = df[(df['country']=='Spain') & ~(df['city']=='Barcelona')]\n",
    "not_bcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique city names\n",
    "not_bcn['city'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where either the city is munich, or the country is serbia\n",
    "df[(df['city']=='Munich') | (df['country']=='Serbia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many observations are west of the prime meridian?\n",
    "len(df[df['lon'] < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all rows that contain a city that starts with the letter G\n",
    "row_mask = df['city'].str.startswith('G')\n",
    "df[row_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows with certain city names by using .isin([list])\n",
    "row_mask = df['city'].isin(['Munich', 'Berat', 'Maia', 'Sarajevo'])\n",
    "df[row_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# select all the rows that are either in Portugal or are east of the prime meridian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with date-time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the location data set, indexed by the date field\n",
    "# and, parse the dates so they're no longer strings but now rather Python datetime objects\n",
    "# this lets us do date and time based operations on the data set\n",
    "dt = pd.read_csv('data/gps-coords.csv', index_col='date', parse_dates=True)\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1759 rows - but is the timestamp index unique?\n",
    "dt.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates where values in all columns match\n",
    "dt = dt.drop_duplicates(inplace=False)\n",
    "len(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now is it unique?\n",
    "dt.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop any rows that duplicate another's index label\n",
    "dt = dt[~dt.index.duplicated(keep='first')]\n",
    "len(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now is it unique?\n",
    "dt.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .weekday attribute determines which day of the week a date is\n",
    "# 0 is monday and 6 is sunday, Tue-Sat are 1-5\n",
    "# what day of the week is each datetime in our dataframe's index?\n",
    "dt.index.weekday[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bitwise OR to create a boolean vector of which rows are on a weekend\n",
    "weekend_mask = (dt.index.weekday==5) | (dt.index.weekday==6)\n",
    "weekend_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekends = dt[weekend_mask]\n",
    "weekdays = dt[~weekend_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_share = pd.DataFrame()\n",
    "\n",
    "# calculate what share of the weekday observations each hour has\n",
    "weekday_hourly = weekdays.groupby(weekdays.index.hour).size()\n",
    "hourly_share['weekday'] = weekday_hourly / weekday_hourly.sum()\n",
    "\n",
    "# calculate what share of the weekend observations each hour has\n",
    "weekend_hourly = weekends.groupby(weekends.index.hour).size()\n",
    "hourly_share['weekend'] = weekend_hourly / weekend_hourly.sum()\n",
    "\n",
    "# format the x-axis ticks like 0:00 times and plot the data\n",
    "hourly_share.index = [s + ':00' for s in hourly_share.index.astype(str)]\n",
    "hourly_share.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it - we'll learn all about this in the next module\n",
    "%matplotlib inline\n",
    "ax = hourly_share.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# select all the rows from the dt DataFrame that occurred between 06:00 AM and 12:00 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge and Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/tracts_pop_age.csv', dtype={'GEOID':str}).set_index('GEOID')\n",
    "df2 = pd.read_csv('data/tracts_white_income.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.shape)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(1000)\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sample(1000)\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes together\n",
    "df = pd.merge(left=df1, right=df2, how='inner', left_index=True, right_on='GEOID')\n",
    "df = df.set_index('GEOID')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you change the \"how\" argument to 'left' or 'right' or 'outer'? How do you explain this behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two subset dataframes\n",
    "df_la = df[df['place_name']=='Los Angeles, CA']\n",
    "df_sf = df[df['place_name']=='San Francisco, CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the subsets back together\n",
    "df_ca = pd.concat([df_la, df_sf], axis=0)\n",
    "df_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create two subsets of the dataframe: one containing tracts in Arizona, and one containing tracts in Florida\n",
    "# then concatenate them back together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Map and Apply\n",
    "\n",
    "These methods are useful for mapping/applying a function across elements, rows, and columns of a pandas DataFrame or Series. But they have some important and often confusing differences.\n",
    "\n",
    "1. `.map()` applies a function element-wise on a Series\n",
    "2. `.apply()` works on a row or column basis on a DataFrame (specify the axis!), or element-wise on a Series\n",
    "3. `.applymap()` works element-wise on an entire DataFrame\n",
    "\n",
    "Let's see what that means in practice with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv('data/gps-coords-reduced.csv')\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse date string to a datetime object\n",
    "dt['date'] = pd.to_datetime(dt['date'])\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can iterate through a DataFrame using the .iterrows() method\n",
    "for label, row in dt.loc[50:60].iterrows():\n",
    "    if row['date'].month > 5:\n",
    "        print('summer')\n",
    "    else:\n",
    "        print('spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alternatively, .map() applies a function element-wise on a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function\n",
    "def get_season(date):\n",
    "    if date.month > 5:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'spring'\n",
    "    \n",
    "# then map it to the series\n",
    "dt['season'] = dt['date'].map(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or do the same thing all in one line, using a lambda function as .map()'s argument\n",
    "# you commonly see this with pandas\n",
    "dt['season'] = dt['date'].map(lambda date: 'summer' if date.month > 5 else 'spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [lambda function](http://www.python-course.eu/lambda.php) is a simple, one-off, anonymous function. You can't call it again later because it doesn't have a name. It just lets you repeatedly perform some operation across a series of values (in our case, a column in our dataframe) using a minimal amount of code. Also notice that the if-else statement is all on one line: this is called a [ternary operator](http://pythoncentral.io/one-line-if-statement-in-python-ternary-conditional-operator/) or an inline-if."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# write a function to determine if the datetime is AM or PM, then map it to the date column\n",
    "# next, rewrite it as a lambda function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .apply() is like .map(), but it works on a row or column basis on an entire DataFrame (specify the axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame with fake year data\n",
    "df = pd.DataFrame({'start_year':[2001, 2002, 2005, 2005, 2006], \n",
    "                   'mid_year':[2002, 2010, 2008, 2006, 2014],\n",
    "                   'end_year':[2012, 2018, 2018, 2016, 2017]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the difference between the min and max values in a series\n",
    "def get_difference(vector):\n",
    "    difference = vector.max() - vector.min()\n",
    "    return difference\n",
    "\n",
    "df.apply(get_difference, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing again, using a lambda function\n",
    "df.apply(lambda x: x.max() - x.min(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between the min and max values in each row (ie, column-wise) and save to a new column\n",
    "df['difference'] = df.apply(get_difference, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .applymap() works element-wise on an entire DataFrame\n",
    "This is like doing a .map() to each column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide every value in the dataframe by two\n",
    "df.applymap(lambda x: x / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracts_white_income.csv', dtype={'GEOID':str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the place_name column with discrete city and state columns\n",
    "df['city'] = df['place_name'].map(lambda x: x.split(', ')[0])\n",
    "df['state'] = df['place_name'].map(lambda x: x.split(', ')[1])\n",
    "df = df.drop(columns=['place_name'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3-level hierarchical index and sort it for fast look-up performance\n",
    "df = df.set_index(['state', 'city', 'GEOID']).sort_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the multiindex unique?\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the top level with a single value\n",
    "df.loc['CA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the top two levels with a tuple\n",
    "df.loc[('CA', 'Los Angeles')].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the top level, group by the 2nd level, calculate a summary stat\n",
    "df.loc['CA'].groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercise\n",
    "\n",
    "  1. Load the tracts_pop_age.csv and the tracts_white_income.csv datasets as DataFrames (do not sample or filter them).\n",
    "  1. Merge them together on the geoid.\n",
    "  1. Map a function to create a new dummy variable with value of 1 if median income is greater than \\$50,000 and 0 otherwise.\n",
    "  1. How many tracts have a non-hispanic white majority of the population?\n",
    "  1. What is the average tract-level median income in Boston?\n",
    "  1. Create a subset of the dataframe that only contains tracts in states with names that begin with the letter \"N\".\n",
    "  1. Create a subset of this subset that only contains tracts with median income below \\$30,000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (urbinf)",
   "language": "python",
   "name": "urbinf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
